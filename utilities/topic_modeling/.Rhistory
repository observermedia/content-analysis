})
#bind together as dataframe and remove NA's
df <- as.data.frame(do.call('rbind',json_file))
df<-na.omit(df)
df$content <- as.character(df$content)
#remove json_file
rm(json_file)
df<- df[sample(nrow(df), 4000), ]
#TODO: Fix so it reads in more than 1 document to corpus
#remove anything between  [ ]
df$content <-gsub("\\[[^\\]]*\\]", "",df$content,perl = TRUE)
#remove html tags from content
df$content <- gsub("<.*?>", "", df$content, perl = TRUE)
#remove newline character
df$content <- gsub("[\r\n]", "", df$content, perl = TRUE)
#wtf.. I need to encode the corpus to UTF-8-MAC? .. Doesnt work on my ubuntu machine.. but works on mac
content<- VCorpus(VectorSource(df$content))
#Currently runs on 2 cores, specificy mc.cores param to adjust
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
content.clean <- tm_map(content,removeNumbers)                                    #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,mystops)                        #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
content.clean <- tm_map(content.clean, stripWhitespace)                           #strip whitespace
#create dtm
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
#removing empty rows from dtm (i.e. empty docs)
#Keep 1-1 correspondance between dtm and corpus
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
df.new<-df[-as.numeric(empty.rows),]
num_topics <- 5
lda_object<- LDA(dtm.new,num_topics)
terms(lda_object,10)
json_file <- fromJSON(file='clean_data_links.json')
json_file <- lapply(json_file,function(x){
x[sapply(x,is.null)]<-NA
unlist(x)
})
#bind together as dataframe and remove NA's
df <- as.data.frame(do.call('rbind',json_file))
df<-na.omit(df)
df$content <- as.character(df$content)
#remove json_file
rm(json_file)
mystops<-read.csv('mystops.csv')
mystops<- mystops$a
df<- df[sample(nrow(df), 4500), ]
#TODO: Fix so it reads in more than 1 document to corpus
#remove anything between  [ ]
df$content <-gsub("\\[[^\\]]*\\]", "",df$content,perl = TRUE)
#remove html tags from content
df$content <- gsub("<.*?>", "", df$content, perl = TRUE)
#remove newline character
df$content <- gsub("[\r\n]", "", df$content, perl = TRUE)
#wtf.. I need to encode the corpus to UTF-8-MAC? .. Doesnt work on my ubuntu machine.. but works on mac
content<- VCorpus(VectorSource(df$content))
#Currently runs on 2 cores, specificy mc.cores param to adjust
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
content.clean <- tm_map(content,removeNumbers)                                    #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,mystops)                        #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
content.clean <- tm_map(content.clean, stripWhitespace)                           #strip whitespace
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
#removing empty rows from dtm (i.e. empty docs)
#Keep 1-1 correspondance between dtm and corpus
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
df.new<-df[-as.numeric(empty.rows),]
num_topics <- 5
lda_object<- LDA(dtm.new,num_topics)
terms(lda_object,10)
require(rjson)
#require(RJSONIO)
require(NLP)
require(tm)
require(topicmodels)
require(SnowballC)
require(LDAvis)
source('topicmodel_utilities.R')
json_file <- fromJSON(file='clean_data_links.json')
json_file <- lapply(json_file,function(x){
x[sapply(x,is.null)]<-NA
unlist(x)
})
#bind together as dataframe and remove NA's
df <- as.data.frame(do.call('rbind',json_file))
df<-na.omit(df)
df$content <- as.character(df$content)
#remove json_file
rm(json_file)
mystops<-read.csv('mystops.csv')
mystops<- mystops$a
df<- df[sample(nrow(df), 4500), ]
#TODO: Fix so it reads in more than 1 document to corpus
#remove anything between  [ ]
df$content <-gsub("\\[[^\\]]*\\]", "",df$content,perl = TRUE)
#remove html tags from content
df$content <- gsub("<.*?>", "", df$content, perl = TRUE)
#remove newline character
df$content <- gsub("[\r\n]", "", df$content, perl = TRUE)
#wtf.. I need to encode the corpus to UTF-8-MAC? .. Doesnt work on my ubuntu machine.. but works on mac
content<- VCorpus(VectorSource(df$content))
#Currently runs on 2 cores, specificy mc.cores param to adjust
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')), mc.cores=1)
content.clean <- tm_map(content,removeNumbers)                                    #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,mystops)                        #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
content.clean <- tm_map(content.clean, stripWhitespace)                           #strip whitespace
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
#removing empty rows from dtm (i.e. empty docs)
#Keep 1-1 correspondance between dtm and corpus
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
df.new<-df[-as.numeric(empty.rows),]
num_topics <- 5
lda_object<- LDA(dtm.new,num_topics)
terms(lda_object,10)
lda_object<- LDA(dtm.new,num_topics)
terms(lda_object,10)
topicnames <- c("politics_B","Entertainment_Culture","Business_tech_internetCulture","Politics_A","Art_leisure")
gammaDF <- as.data.frame(lda_object@gamma)
names(gammaDF) <- c(topicnames) #assign topic names here
toptopics <- as.data.frame(cbind(document = row.names(gammaDF),
topic = apply(gammaDF,1,function(x) names(gammaDF)[which(x==max(x))])))
df.new<-cbind(df.new,toptopics$topic)
df.new$topic <- df.new$'toptopics$topic'
save.image("~/Dropbox/Upwork/Observer/Clean_data/primary_topics/LDA_5.RData")
df<-cbind(df,gammaDF)
df<-cbind(df.new,gammaDF)
View(df)
nb_topics <- 5
topicnames <-c('Politics_B1','Politics_B2','Politics_B3','Politics_B4','Politics_B5')
subtopic <- "Politics_B"
df <- df.new[df.new$topic == subtopic,]
View(df.new)
nb_topics <- 5
topicnames <-c('Politics_B1','Politics_B2','Politics_B3','Politics_B4','Politics_B5')
subtopic <- "politics_B"
df <- df.new[df.new$topic == subtopic,]
#create dtm
content<- VCorpus(VectorSource(df$content))
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')), mc.cores=16)
content.clean <- tm_map(content, stripWhitespace)                                 #strip whitespace
content.clean <- tm_map(content.clean,removeNumbers)                              #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,stopwords("english"))           #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
#create dtm
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
dfsub.new<-df[-as.numeric(empty.rows),]
lda_object<- LDA(dtm,nb_topics)
setwd("~/Dropbox/Upwork/Observer/lda_viz/subtopics")
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = paste('viz_sub',subtopic,sep = ''))
setwd("~/Dropbox/Upwork/Observer/content-analysis/utilities/topic_modeling")
source('topicmodel_utilities.R')
setwd("~/Dropbox/Upwork/Observer/lda_viz/subtopics")
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = paste('viz_sub',subtopic,sep = ''))
gammaDF_sub <- as.data.frame(lda_object@gamma)
names(gammaDF_sub) <- c(topicnames) #assign to
tmp<- cbind(gammadf_sub,dfsub.new)
tmp<- cbind(gammaDF_sub,dfsub.new)
tmp<- cbind(gammaDF_sub,df)
temp_df<-join(tmp,df.new)
View(temp_df)
View(df.new)
temp_df<-join(df.new,temp)
temp_df<-join(df.new,tmp)
View(temp_df)
nb_topics <- 5
subtopic <- "Entertainment_Culture"
nb_topics <- 5
subtopic <- "Entertainment_Culture"
topicnames <-c('Entertainment_Culture1','Entertainment_Culture2','Entertainment_Culture3','Entertainment_Culture4','Entertainment_Culture5')
df <- df.new[df.new$topic == subtopic,]
#create dtm
content<- VCorpus(VectorSource(df$content))
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')), mc.cores=16)
content.clean <- tm_map(content, stripWhitespace)                                 #strip whitespace
content.clean <- tm_map(content.clean,removeNumbers)                              #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,stopwords("english"))           #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
#create dtm
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
dfsub.new<-df[-as.numeric(empty.rows),]
#topic model
lda_object<- LDA(dtm,nb_topics)
#output viz
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = paste('viz_sub',subtopic,sep = ''))
gammaDF_sub <- as.data.frame(lda_object@gamma)
names(gammaDF_sub) <- c(topicnames) #assign topic names here
tmp<- cbind(gammaDF_sub,df)
temp_df<-join(temp_df,tmp)
nb_topics <- 5
subtopic <- "Business_tech_internetCulture"
topicnames <-c('Business_tech_internetCulture1','Business_tech_internetCulture2','Business_tech_internetCulture3','Business_tech_internetCulture4','Business_tech_internetCulture5')
df <- df.new[df.new$topic == subtopic,]
#create dtm
content<- VCorpus(VectorSource(df$content))
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')), mc.cores=16)
content.clean <- tm_map(content, stripWhitespace)                                 #strip whitespace
content.clean <- tm_map(content.clean,removeNumbers)                              #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,stopwords("english"))           #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
#create dtm
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
dfsub.new<-df[-as.numeric(empty.rows),]
#topic model
lda_object<- LDA(dtm,nb_topics)
#output viz
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = paste('viz_sub',subtopic,sep = ''))
gammaDF_sub <- as.data.frame(lda_object@gamma)
names(gammaDF_sub) <- c(topicnames) #assign topic names here
tmp<- cbind(gammaDF_sub,df)
temp_df<-join(temp_df,tmp)
nb_topics <- 5
subtopic <- "Politics_A"
topicnames <-c('Politics_A1','Politics_A2','Politics_A3','Politics_A4','Politics_A5')
df <- df.new[df.new$topic == subtopic,]
#create dtm
content<- VCorpus(VectorSource(df$content))
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')), mc.cores=16)
content.clean <- tm_map(content, stripWhitespace)                                 #strip whitespace
content.clean <- tm_map(content.clean,removeNumbers)                              #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,stopwords("english"))           #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
#create dtm
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
dfsub.new<-df[-as.numeric(empty.rows),]
#topic model
lda_object<- LDA(dtm,nb_topics)
#output viz
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = paste('viz_sub',subtopic,sep = ''))
gammaDF_sub <- as.data.frame(lda_object@gamma)
names(gammaDF_sub) <- c(topicnames) #assign topic names here
tmp<- cbind(gammaDF_sub,df)
temp_df<-join(temp_df,tmp)
nb_topics <- 5
subtopic <- "Art_leisure"
topicnames <-c('Art_leisure1','Art_leisure2','Art_leisure3','Art_leisure4','Art_leisure5')
df <- df.new[df.new$topic == subtopic,]
#create dtm
content<- VCorpus(VectorSource(df$content))
content <- tm_map(content,content_transformer(function(x) iconv(x, to='UTF-8', sub='byte')), mc.cores=16)
content.clean <- tm_map(content, stripWhitespace)                                 #strip whitespace
content.clean <- tm_map(content.clean,removeNumbers)                              #remove numbers
content.clean <- tm_map(content.clean,removePunctuation)                          #remove punctuation
content.clean <- tm_map(content.clean,content_transformer(tolower))               #convert to lower case
content.clean <- tm_map(content.clean,removeWords,stopwords("english"))           #removestop words
content.clean <- tm_map(content.clean,stemDocument)                               #stem words
#create dtm
dtm <- DocumentTermMatrix(content.clean, control = list(weighting = weightTf))
rowsums <- apply(dtm,1,sum)
empty.rows <- dtm[rowsums == 0, ]$dimnames[1][[1]]
dtm.new <- dtm[rowsums > 0, ]
content.new <- content[-as.numeric(empty.rows)]
dfsub.new<-df[-as.numeric(empty.rows),]
#topic model
lda_object<- LDA(dtm,nb_topics)
#output viz
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = paste('viz_sub',subtopic,sep = ''))
gammaDF_sub <- as.data.frame(lda_object@gamma)
names(gammaDF_sub) <- c(topicnames) #assign topic names here
tmp<- cbind(gammaDF_sub,df)
temp_df<-join(temp_df,tmp)
View(temp_df)
temp_df<-cbind(temp_df,gammaDF)
View(temp_df)
save.image("~/Dropbox/Upwork/Observer/Clean_data/primary_topics/LDA_5.RData")
json_file <- fromJSON(file='clean_data_links.json')
json_file <- lapply(json_file,function(x){
x[sapply(x,is.null)]<-NA
unlist(x)
})
#bind together as dataframe and remove NA's
df <- as.data.frame(do.call('rbind',json_file))
df<-na.omit(df)
df$content <- as.character(df$content)
#remove json_file
rm(json_file)
setwd("~/Dropbox/Upwork/Observer/Clean_data")
json_file <- fromJSON(file='clean_data_links.json')
json_file <- lapply(json_file,function(x){
x[sapply(x,is.null)]<-NA
unlist(x)
})
#bind together as dataframe and remove NA's
df <- as.data.frame(do.call('rbind',json_file))
df<-na.omit(df)
df$content <- as.character(df$content)
#remove json_file
rm(json_file)
num_images <- 0
for (i in 1:nrow(df)){
num_images[i]<-sum(str_count( df$content[i], "img class="))
}
df$num_images <- num_images
library("stringr", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
num_images <- 0
for (i in 1:nrow(df)){
num_images[i]<-sum(str_count( df$content[i], "img class="))
}
df$num_images <- num_images
num_links <- 0
for (i in 1:nrow(df)){
num_links[i]<-sum(str_count( df$content[i], "<a href="))
}
df$num_links <- num_links
sent_tmp <- read.csv('sent_scores.csv')
sent_tmp<- sent_tmp[,c(4:8)]
fles_tmp <- read.csv('word_complex_scores.csv')
fles_tmp <- fles_tmp[,c(4,5)]
View(df)
wiki_links <- 0
for (i in 1:nrow(df)){
wiki_links[i]<-sum(str_count( df$content[i], "<a href=https://en.wikipedia"))
}
sum(wiki_links)
wiki_links <- 0
for (i in 1:nrow(df)){
wiki_links[i]<-sum(str_count( df$content[i], "<a href= https://en.wikipedia"))
}
sum(wiki_links)
sent_tmp[1]
wiki_links <- 0
for (i in 1:nrow(df)){
wiki_links[i]<-sum(str_count( df$content[i], '<a href="https://en.wikipedia'))
}
sum(wiki_links)
df$wiki_links <- wiki_links
merge_tmp<- as.data.frame(cbind(df$num_images,df$num_links,df$wiki_links,as.character(df$link)))
colnames(merge_tmp) <- c('num_images','num_links','wiki_links','Page')
merge_tmp$Page<-gsub("http://observer.com", "", merge_tmp$Page ,perl = TRUE)
sent_tmp$Page <- gsub("http://observer.com", "", sent_tmp$Page ,perl = TRUE)
fles_tmp$Page <- gsub("http://observer.com", "", fles_tmp$Page ,perl = TRUE)
merge1<- merge(x=merge_tmp,y=sent_tmp, by = "Page")
merge2<- merge(x=merge1,y=fles_tmp, by = "Page")
View(merge2)
wiki_links
bindme <- temp_df
bindme$Page<-gsub("http://observer.com", "", bindme$link ,perl = TRUE)
bindme<- merge(x=bindme,y=merge2, by = "Page")
View(bindme)
setwd("~/Dropbox/Upwork/Observer/content_reports")
y14 <- read.csv('Content Report - Top 2500 - 20140101-20141231.csv', skip=6)
y15 <- read.csv('Content Report - Top 2500 - 20150101-20151231.csv', skip=6)
y16 <- read.csv('Content Report - Top 2500 - 20160101-20160930.csv', skip=6)
View(y14)
consumer_reports<- rbind(y14,y15,y16)
consumer_reports<-consumer_reports[!duplicated(consumer_reports$Page), ]
modelme <- merge(x=consumer_reports, y=bindme, by = "Page")
modelme$Pageviews <- as.numeric(gsub(",","",modelme$Pageviews))
modelme$WordCount <-sapply(gregexpr("\\W+", modelme$content), length) + 1
modelme$WordCountTitle <- sapply(gregexpr("-+", modelme$title), length) + 1
time <- modelme$Avg..Time.on.Page
minute<- 0
second<-0
i<-1
for (i in 1:length(time)){
minute[i]<-as.numeric(unlist(strsplit(as.character(time[i]), split=':', fixed=TRUE))[2])
second[i]<-as.numeric(unlist(strsplit(as.character(time[i]), split=':', fixed=TRUE))[3])
}
modelme$time<- (minute*60) + second
View(modelme)
save.image("~/Dropbox/Upwork/Observer/Clean_data/modeling_5topics_local.RData")
nums <- sapply(modelme, is.numeric)
modelme2<- modelme[,nums]
library(caret)
library(coefplot)
install.packages("coefplot")
library(coefplot)
nums <- sapply(modelme, is.numeric)
modelme2<- modelme[,nums]
#Zero-fill NA values
modelme2[is.na(modelme2)] <- 0
#Look for variable with near zero variance
#No variables are nzv...looks good here
nzv <- nearZeroVar(modelme2, saveMetrics= TRUE)
#Look at correlated predictors
#bad for regression models good for other types of models
#looks good here.. no variables with correlation > .75
descrCor <-  cor(modelme2)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
#Look at variables with linear-dependence (i.e. co-linearlity)
#bad for regression models
comboInfo <- findLinearCombos(modelme2)
comboInfo #suggests remove column 45 (i.e. NY_Pol5)
modelme2<-modelme2[, -comboInfo$remove]
#Center and scale (normalize) all variables
#intercept term (in LR) will now be interpreted as the expected value of Y
preProcValues <- preProcess(modelme2, method = c("center", "scale"))
modelme2<- predict(preProcValues, modelme2)
#Filter out all articles where page views is greater than 3SDs away from mean
#basically removing outliers
modelme2 <- modelme2[!modelme2$Pageviews > (mean(modelme2$Pageviews) + 3*sd(modelme2$Pageviews)),]
modelme2 <- modelme2[!modelme2$Pageviews < (mean(modelme2$Pageviews) -  3*sd(modelme2$Pageviews)),]
Step wise model
min.model <- lm(Pageviews ~ 1, data=modelme2)
biggest <- formula(lm(Pageviews~.,modelme2))
step(min.model, direction='forward', scope=biggest)
regPageViews2 <- lm(  Pageviews ~ Politics_A + WordCount + X..Exit + Business_tech_internetCulture +
Politics_A2 + Politics_A1 + Bounce.Rate + time, data = modelme2))
regPageViews2 <- lm(  Pageviews ~ Politics_A + WordCount + X..Exit + Business_tech_internetCulture +
Politics_A2 + Politics_A1 + Bounce.Rate + time, data = modelme2)
summary(regPageViews2)
coefplot(regPageViews2, "Page Views")
min.model <- lm(time ~ 1, data=modelme2)
biggest <- formula(lm(time~.,modelme2))
step(min.model, direction='forward', scope=biggest)
step(min.model, direction='forward', scope=biggest)
regAvgTime2 <- lm(time ~ WordCount + Business_tech_internetCulture +
X..Exit + Bounce.Rate + politics_B + flesh_score + Business_tech_internetCulture5 +
Politics_B2 + neg_score + Art_leisure2 + Politics_B4 + Entertainment_Culture2 +
Pageviews + Entertainment_Culture5 + Art_leisure3 + Politics_A +
Politics_A2, data = modelme2)
summary(regAvgTime2)
coefplot(regAvgTime2,title="Avg Time on Page")
summary(regPageViews2)
summary(regAvgTime2)
max(modelme$Politics_A2)
max(modelme$Politics_A2,na.rm=TRUE)
order(modelme$Politics_A2,decreasing=T)[1]
modelme[order(modelme$Politics_A2,decreasing=T)[1],]
modelme[order(modelme$Politics_A2,decreasing=T)[1],]$link
modelme[order(modelme$Politics_A2,decreasing=T)[1:5],]$link
print(modelme[order(modelme$Politics_A2,decreasing=T)[1:5],]$link)
print(modelme[order(modelme$Art_leisure3,decreasing=T)[1:5],]$link)
print(modelme[order(modelme$Entertainment_Culture5,decreasing=T)[1:5],]$link)
print(modelme[order(modelme$Politics_B4,decreasing=T)[1:5],]$link)
print(modelme[order(modelme$Art_leisure2,decreasing=T)[1:5],]$link)
print(modelme[order(modelme$Politics_B2,decreasing=T)[1:5],]$link)
print(modelme[order(modelme$Business_tech_internetCulture5,decreasing=T)[1:5],]$link)
coefplot(regAvgTime2,title="Avg Time on Page")
regAvgTime2 <- lm(time ~ WordCount + Business_tech_internetCulture +
X..Exit + Bounce.Rate + politics_B + flesh_score + Business_tech_internetCulture5 +
Politics_B2 + neg_score + Art_leisure2 + Politics_B4 + Entertainment_Culture2 +
Pageviews + Entertainment_Culture5 + Art_leisure3 + Politics_A +
Politics_A2, data = modelme2)
summary(regAvgTime2)
summary(regPageViews2)
coefplot(regPageViews2, "Page Views")
print(modelme[order(modelme$Politics_A1,decreasing=T)[1:5],]$link)
print(modelme[order(modelme$Politics_A2,decreasing=T)[1:5],]$link)
View(modelme)
getwd()
setwd("~/Dropbox/Upwork/Observer/Clean_data")
write.csv(modelme,'model_dataframe.csv')
load("~/Dropbox/Upwork/Observer/Clean_data/modeling_5topics_local.RData")
setwd("~/Dropbox/Upwork/Observer/content-analysis/utilities/topic_modeling")
require(rjson)
#require(RJSONIO)
require(NLP)
require(tm)
require(topicmodels)
require(SnowballC)
require(LDAvis)
source('topicmodel_utilities.R')
vizme <- topicmodels_json_ldavis(lda_object,content.new,dtm.new)
vizme <- topicmodels_json_ldavis(lda_object,content.new,dtm.new)
terms(lda_object,10)
topics(lda_object, 5)[,1:5]
vizme <- topicmodels_json_ldavis(lda_object,content.new,dtm.new)
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = 'viz_topics')
install.packages('servr')
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = 'viz_topics')
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = 'viz_5_Primarytopics')
vizme <- topicmodels_json_ldavis(lda_object,content,dtm)
serVis(vizme,out.dir = 'viz_5_Primarytopics')
serVis(vizme,out.dir = 'viz_5_Primarytopics')
